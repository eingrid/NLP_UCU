{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2660788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/ubertext.social.filter_rus_gcld+short.text_only.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 40963744 types 1943894\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:23326728\n",
      "Statistics:\n",
      "1 1943894 D1=0.731735 D2=1.01202 D3+=1.29701\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 81 assuming -p 1.5\n",
      "probing 88 assuming -r models -p 1.5\n",
      "trie    56 without quantization\n",
      "trie    51 assuming -q 8 -b 8 quantization \n",
      "trie    56 assuming -a 22 array pointer compression\n",
      "trie    51 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:23326728\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:23326728\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:26323396 kB\tVmRSS:61980 kB\tRSSMax:8789652 kB\tuser:5.21467\tsys:1.69986\tCPU:6.91455\treal:6.95047\n"
     ]
    }
   ],
   "source": [
    "!./kenlm_bin//lmplz -o 1 < ubertext.social.filter_rus_gcld+short.text_only.txt > models/baseline1.arpa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/ubertext.social.filter_rus_gcld+short.text_only.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 40963744 types 1943894\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:23326728 2:26761897984\n",
      "Statistics:\n",
      "1 1943894 D1=0.750817 D2=1.02444 D3+=1.28531\n",
      "2 13607084 D1=0.769403 D2=1.09748 D3+=1.35753\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 281 assuming -p 1.5\n",
      "probing 289 assuming -r models -p 1.5\n",
      "trie    128 without quantization\n",
      "trie     91 assuming -q 8 -b 8 quantization \n",
      "trie    128 assuming -a 22 array pointer compression\n",
      "trie     91 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:23326728 2:217713344\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:23326728 2:217713344\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:26323396 kB\tVmRSS:62108 kB\tRSSMax:7395172 kB\tuser:14.7704\tsys:1.79638\tCPU:16.5668\treal:16.3462\n"
     ]
    }
   ],
   "source": [
    "!./kenlm_bin/lmplz -o 2 < ubertext.social.filter_rus_gcld+short.text_only.txt > models/baseline2.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf9617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/ubertext.social.filter_rus_gcld+short.text_only.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 40963744 types 1943894\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:23326728 2:9308486656 3:17453412352\n",
      "Statistics:\n",
      "1 1943894 D1=0.750817 D2=1.02444 D3+=1.28531\n",
      "2 13607084 D1=0.797981 D2=1.13591 D3+=1.37188\n",
      "3 26219186 D1=0.835598 D2=1.20415 D3+=1.43833\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 809 assuming -p 1.5\n",
      "probing 894 assuming -r models -p 1.5\n",
      "trie    383 without quantization\n",
      "trie    235 assuming -q 8 -b 8 quantization \n",
      "trie    356 assuming -a 22 array pointer compression\n",
      "trie    208 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:23326728 2:217713344 3:524383720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:23326728 2:217713344 3:524383720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:26323392 kB\tVmRSS:62252 kB\tRSSMax:6612712 kB\tuser:25.8568\tsys:3.45423\tCPU:29.311\treal:25.5771\n"
     ]
    }
   ],
   "source": [
    "!./kenlm_bin/lmplz -o 3 < ubertext.social.filter_rus_gcld+short.text_only.txt > models/baseline3.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from src.utils import process_dataset, evaluate_accuracy\n",
    "from src.wrapper import get_model\n",
    "\n",
    "samples = []\n",
    "with open('ua_asr_hypotheses_500.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    samples.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8dfde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: models/baseline1.arpa\n",
      "Accuracy: 0.5920\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/models/baseline2.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Loading the LM will be faster if you build a binary file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: models/baseline2.arpa\n",
      "Accuracy: 0.7440\n",
      "-----------------------\n",
      "Evaluating model: models/baseline3.arpa\n",
      "Accuracy: 0.7580\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/models/baseline3.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "model_paths = [\n",
    "    'models/baseline1.arpa',\n",
    "    'models/baseline2.arpa',\n",
    "    'models/baseline3.arpa'\n",
    "]\n",
    "for path in model_paths:\n",
    "    model = get_model(path)\n",
    "    res = process_dataset(model,samples)\n",
    "    print(f'Evaluating model: {path}')\n",
    "    accuracy = evaluate_accuracy(res, samples)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd5bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: models/baseline1.arpa\n",
      "Accuracy: 0.6020\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/models/baseline2.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: models/baseline2.arpa\n",
      "Accuracy: 0.7340\n",
      "-----------------------\n",
      "Evaluating model: models/baseline3.arpa\n",
      "Accuracy: 0.7740\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/nazara/UCU/NLP/ASSIGNMENT1/models/baseline3.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "model_paths = [\n",
    "    'models/baseline1.arpa',\n",
    "    'models/baseline2.arpa',\n",
    "    'models/baseline3.arpa'\n",
    "]\n",
    "for path in model_paths:\n",
    "    model = get_model(path)\n",
    "    res = process_dataset(model,samples, use_perplexity=False)\n",
    "    print(f'Evaluating model: {path}')\n",
    "    accuracy = evaluate_accuracy(res, samples)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print('-----------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1 (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
